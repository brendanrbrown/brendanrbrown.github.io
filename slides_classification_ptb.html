<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>models for classification, part b</title>
    <meta charset="utf-8" />
    <meta name="author" content="" />
    <link href="slides_classification_ptb_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slides_classification_ptb_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="slides_classification_ptb_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# models for classification, part b

---







# What you will learn

- basics of Support Vector Machine (SVM) classification
- using the `svm` function in the `e1071` package


**Main reference:** ISLR ch 9

**Resources**

- [SVM demo](http://uc-r.github.io/svm)

---
# Review: Logistic regression

`\(Y_i = 0\)` or `\(1\)` outcome, with explanatory/regressor/predictor variables `\(X_{i1} \ldots X_{ip}\)`. Write `\(X_i\)` for the vector of variables for observation `\(i\)`---this is the `\(i\)`th row of your data frame.

Linear regression no longer makes sense. Instead estimate `\(p_i =\)` Probability `\((Y_i = 1)\)` with `$$p_i = \frac{\exp(\beta_0 + \sum_{j=1}^p X_{ij} \beta_j) }{1+\exp(\beta_0 + \sum_{j=1}^p X_{ij} \beta_j) }$$`

### Extension for multiple categories
Use the `multinom` function in the `nnet` package. The idea is the same, this time we estimate the probabilities Prob$(Y_i = m)$ for categories `\(m = 1\ldots M\)`.

I won't cover this, but the `multinom` syntax and results should be familiar. Some resources:

- [Multinomial logistic regression example](https://rpubs.com/heruwiryanto/multinom-reg)
- [nnet package documentation](https://cran.r-project.org/web/packages/nnet/nnet.pdf)

---
# Today: Anti social-distancing
Not *anti-social distancing*, which is what we all should be doing.

### Remember: k-means
In the k-means algorithm we

- imagine data vectors `\(X_i = (X_{i1}, \ldots X_{ip})\)` as living in `\(p\)`-dimensional space
- observations close to each other are grouped together
- **unsupervised:** no `\(Y_i\)` representing the true category of `\(i\)`th observation

### Supervised: Same category if close to each other
There are several supervised methods using the idea that closeness means sameness in different ways. We will explore one today.

In this case `\(Y_i\)` will again represent the category or class of observation `\(i\)`, with possible values `\(Y_i = 0, 1\)`. 

Aside: In the textbook section on SVM, `\(Y_i = -1, 1\)` but these labels don't matter.
---
# Which one is different?

![](https://m.media-amazon.com/images/M/MV5BMTUxNDM4Njc4Ml5BMl5BanBnXkFtZTYwNjIzMjQ2._V1_.jpg)

---
# Support Vector Machine (SVM)
Will divide the data into two classes by 

- **separating points with hyperplanes**
- with **wiggle room** for cases where points can't be perfectly separated
- think of a hyperplane as a sheet of paper in 3d space
- or more generally a 'flat' `\(p-1\)` dimensional space inside our `\(p\)` dimensional variable space

### Optimization problem
It is a little more complicated to state mathematically than I want for the class. See ISLR ch 9.2.2. **The idea is**

- hyperplanes are given by linear equations with coefficients `\(\beta_0 \ldots \beta_p\)`
- find the hyperplane, specifically the coefficients, that **maximizes the distance between the plane and the data points**
- and *almost* separates observations in the different categories
- as much as possible, within a user-determined level of wiggle room allowing some points to be on the wrong side of the

---
# These points can be separated
![](http://uc-r.github.io/public/images/analytics/svm/unnamed-chunk-4-1.png)

from this [SVM demo](http://uc-r.github.io/svm)

---
# These points can't

![](http://uc-r.github.io/public/images/analytics/svm/unnamed-chunk-7-1.png)

---
# Syntax


```
svm(y ~ x1 + x2 + ..., data = df, cost = cost_value, kernel = "linear")
```

Use the `svm` function in the `e1071` package.

`y`, `x1` etc. are placeholders for your outcome `\(Y\)` and variables respectively. `y` must be a factor here.

### On `cost` and `kernel`
Here `cost_value` is a placeholder for an important parameter in the SVM model.

The **cost** parameter determines how much 'wiggle room' for observations on the wrong side of the boundary that we allow in finding the optimal hyperplane. You must determine this. We will see soon how to do so.

`kernel = "linear"` tells the SVM we are looking for hyperplanes to separate the data. This is **not the default**. We'll talk about other options later.

---
# OJ example
Notice `Purchase` must be a factor. `fct_relevel` to make this consistent with data from last class, where the first level 0 represented Minute Maid purchases. The [`forcats` package](https://forcats.tidyverse.org/reference/index.html) has some tools for working with factors.

```r
library(ISLR)
library(tidyverse)
library(e1071)
d &lt;- mutate(OJ, Purchase = fct_relevel(Purchase, "MM"))
```

Trying `cost = 1` and `cost = .01` arbitrarily for comparison

```r
m &lt;- svm(Purchase ~ PriceDiff + LoyalCH, data = d, cost = 1, kernel = "linear")
m2 &lt;- svm(Purchase ~ PriceDiff + LoyalCH, data = d, cost = .01, kernel = "linear")
```

---
# Results

### `svm` class object


```r
class(m)
```

```
## [1] "svm.formula" "svm"
```


```r
names(m)
```

```
##  [1] "call"            "type"            "kernel"          "cost"           
##  [5] "degree"          "gamma"           "coef0"           "nu"             
##  [9] "epsilon"         "sparse"          "scaled"          "x.scale"        
## [13] "y.scale"         "nclasses"        "levels"          "tot.nSV"        
## [17] "nSV"             "labels"          "SV"              "index"          
## [21] "rho"             "compprob"        "probA"           "probB"          
## [25] "sigma"           "coefs"           "na.action"       "fitted"         
## [29] "decision.values" "terms"
```

---
# Less interpretable than linear model
Which of these do we care about?

### Coefficients

- coefficients of the **optimal hyperplane**, unlike those of `glm` or `lm`
- not as interpretable
- if you want to see them, use `coef(m)` **not `m$coef`**

### Visualize
- easy to visualize points separated by a line if `\(p=2\)`
- if `\(p\)` large, look at how the separates data along pairs of variables

### SVM really is a model for prediction, not exploration

- interpretation of variable effects on outcome is hard
- prediction problems care less about detailed model interpretation as in `lm`

**Will see soon how to evaluate models in prediction setting.**
---
# Fit statistics

### `\(\hat{Y}\)`

- SVM splits data into two sections using a hyperplane based on variables `\(X_{i1} \ldots X_{ip}\)`
- Set `\(\hat{Y_i}\)` based on which side of the hyperplane `\(X_{i1} \ldots X_{ip}\)` falls in
- **use the `predict` function**  as usual to get `\(\hat{Y_i}\)`

### How well do we classify?

- error is 1 if `\(\hat{Y_i} \neq Y_i\)` and zero otherwise
- can still ask this question even if the model is not interpretable
- **not the same** as predicting on a new dataset
- since for now we compare `\(Y_i\)` actual outcome with `\(\hat{Y_i}\)` estimated outcome for 

---
### Fit for cost 1 vs. .01
Looking at the proportion of correct `\(\hat{Y}\)`

```r
mutate(d, yhat1 = predict(m), yhat2 = predict(m2),
            err1 = yhat1 != Purchase, err2 = yhat2 != Purchase) %&gt;%
  summarise(accuracy_m =  mean(!err1), accuracy_m2 =mean(!err2))
```

```
##   accuracy_m accuracy_m2
## 1  0.8327103   0.8317757
```

#### This is comparable to the `glm` model we ran last time

---
# Boundary for cost 1

```r
plot(m, d, LoyalCH ~ PriceDiff)
```

![](slides_classification_ptb_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;


---
# Boundary for cost .01

```r
plot(m2, d, LoyalCH ~ PriceDiff)
```

![](slides_classification_ptb_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;


---
# Now you try!

&lt;img src="https://media.giphy.com/media/K2A5bJ3MiZRzW/giphy.gif" height="450" /&gt;


---
# Tutorial
In this tutorial we will explore the `kernel` option, and will run `svm` models on the forest fire data from last time.


```r
park &lt;- read_csv('http://www.dsi.uminho.pt/~pcortez/forestfires/forestfires.csv')
```


1. **Load** the `park` dataset as above.
2. **Create a new variable** called `fire` such that `fire = 1` if `area &gt; 0` and `fire=0` otherwise. Make it a **factor**
3. **Convert** the following variables to `factor` type, to tell `glm` these are categorical:
  - `X`, `Y`, `month`, `day`
  - ideally using a `mutate_at` statement
4. **Run `svm` with radial kernel** and `cost = 10`, `gamma = .1` on the outcome `fire`, using all other variables **except `area`** as predictors.
5. **Plot the output for variables `temp` and `wind`**. Hint: will need `temp ~ wind` statement in the `plot` function.
6. **Calculate and display** the accuracy as calculated above.
7. **Write** one or two sentences describing what you see in the plot vs. the accuracy measure.



---


### What's up with the radial kernel?
Rather than trying to find the optimal *hyperplane* separating categories, **the radial kernel allows for non-linear boundaries** between them.

**This might work well if your data are grouped, for example, in a circular or other non-linear pattern.**

For details, see ISLR 9.3.2.

### radial kernel in `svm`
See `?svm` for syntax.

- `gamma_value` is yet another parameter value that you need to pick
- the **larger the `gamma_value` the less like hyperplanes your boundary between classes can be**



---
# Montesinho Natural Park, Portugal
![](http://www.visitportoandnorth.travel/var/porto_norte/storage/images/porto-and-the-north/visit/artigos/montesinho-natural-park/940900-2-eng-US/Montesinho-Natural-Park.jpg)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
